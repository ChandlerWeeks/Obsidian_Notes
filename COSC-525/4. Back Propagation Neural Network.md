1. Gradient Descent
2. Perceptron (single-layer NN) and Multi-Layer Perceptron
3. Back Propagation (BP)

What we need to understand is the procedure for gradient descent; given a problem in GD, we could solve it. Finds local minimum given a learning rate. Descends towards a minima. 

1. Take an $f(x) = x^2*cos(x)$
2. gradient is f'(x) = $2xcos(x)-x^2sin(x)$
3.  $f'= ...$
4. $x_0 = 0.1$
5. $x_1 = x_0 \frac{f'(x)}{f''(x)} = 0.05$
6. $|x_1-x_0| = 0.05 < \epsilon$ 
7. $x_2 = ...$ until smaller than $\epsilon$ (convergence) 

For gradient descent, c is the learning rate. This learning rate is generally very small