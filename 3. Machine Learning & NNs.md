# Back Propagation Neural Networks (BPNN)
We learn most effectively from mistakes that we make, to avoid or reduce mistakes in the future. The mistake is propagated backwards; use this mistake to update model parameters. 

## Gradient Descent 
Gradient Descent: Using a formula, like a quadratic equation we can find local minimum and maximum by finding the derivative, and setting it to 0. Look into Newton-Raphson method. This finds the roots, which are f(x) = 0.

Newton-Raphson Method
f(x) = x^2 * cos(x)
x1 = -4, x2 = 1
f(x) = 2x*cos(x) - x^2 sin(x)